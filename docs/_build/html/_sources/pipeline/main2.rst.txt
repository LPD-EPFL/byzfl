Main Module
===========

The **main** module serves as the entry point for the Byzantine Machine Learning Framework. It handles the loading of configurations and initiates the training process.

.. automodule:: main
   :members:
   :undoc-members:
   :show-inheritance:

Usage
=====

# Framework for Byzantine ML
This tool facilitates the testing of aggregations and attacks by simulating distributed machine learning environments in a fully configurable manner via the `settings.json` file.

# Requirements
1. Install the dependencies listed in `requirements.txt`.
2. Set the environment variable `CUBLAS_WORKSPACE_CONFIG=:4096:8`.

# Usage or Pipeline
0. **If this is your first time running the framework, execute `python main.py`. This will create a default `settings.json` file, which you can customize according to your requirements.**
1. **Configure the experiment you wish to run in the `settings.json` file. Refer to the above section on how to configure it.**
2. **Run the framework with `python main.py --nb_jobs n`, where `n` represents the number of trainings to be conducted in parallel.**
3. **When the code has finished executing, run `python evaluate_results.py` to choose the best hyperparameters and create heatmaps with the results.**

# How to configure settings.json
- **General**:
    - **training_seed**: Starting training seed for the different seeds that will be used.
    - **nb_training_seeds**: How many seeds will be used starting from the `training_seed`.
    - **device**: Device used in PyTorch to perform computations with torch tensors.
    - **nb_workers**: Number of workers for the training. If null, this number will be `nb_honest + nb_byz`.
    - **nb_honest**: Number of honest clients.
    - **nb_byz**: Number of real Byzantine clients.
    - **declared_nb_byz**: Number of Byzantine clients the server is protecting against.
    - **declared_equal_real**: Boolean to filter configurations that don't have the same declared Byzantine clients as the real ones.
    - **size_train_set**: Proportion of the training set that will be used for training; otherwise, it will be used as the validation set.
    - **nb_steps**: Number of iterations that the learning algorithm will perform.
    - **evaluation_delta**: Number of steps between evaluations of the model over the validation or test set.
    - **evaluate_on_test**: Boolean to configure if test accuracy should be computed.
    - **store_training_accuracy**: Boolean to specify if training accuracies will be stored.
    - **store_training_loss**: Boolean to specify if training losses will be stored.
    - **store_models**: Boolean to specify if the state dict of the PyTorch models should be stored every evaluation delta.
    - **batch_size_validation**: Batch size for the validation and test datasets, but not for the training set.
    - **data_folder**: Location where the datasets will be stored.
    - **results_directory**: Directory where the desired training information will be stored.

- **Model**:
    - **name**: Name of the model to be used in training. This name must correspond to a class in the **models.py** file.
    - **dataset_name**: Name of the dataset used in training. Must be defined in **dataset.py**.
    - **nb_labels**: Number of different targets in the used dataset.
    - **data_distribution_seed**: Starting data distribution seed for the different seeds that will be used. This seed is used for different data splits among nodes.
    - **nb_data_distribution_seeds**: Number of seeds starting from the `data_distribution_seed`.
    - **data_distribution**:
        - **name**: Name of the data distribution to be used.
        - **distribution_parameter**: Float parameter used by the distribution.
    - **loss**: PyTorch training loss.

- **Aggregator**:
    - **name**: Name of the aggregation used by the server. Must be defined in **aggregators.py**.
    - **parameters**: Dictionary with the parameters required by the aggregation used.

- **PreAggregators**: List of pre-aggregations in the order they should be applied.
    - **name**: Name of the pre-aggregation used by the server. Must be defined in **preaggregators.py**.
    - **parameters**: Dictionary with the parameters required by the pre-aggregations used.

- **Server**:
    - **batch_norm_momentum**: Momentum for the federated batch normalization.
    - **learning_rate_decay**: Factor of learning rate decay to be applied at each milestone.
    - **milestones**: List of steps at which the learning rate decay should be applied.

- **Honest Nodes**:
    - **momentum**: Momentum value used in the training algorithm.
    - **batch_size**: Batch size for the SGD algorithm.
    - **learning_rate**: Learning rate value.
    - **weight_decay**: Weight decay value to prevent overfitting.

- **Attack**:
    - **name**: Name of the attack used by the server. Must be defined in **attacks.py**.
    - **parameters**: Dictionary with the parameters required by the attack used.
    - **attack_optimizer**:
        - **name**: Name of the optimizer used to optimize the attack factor.
        - **parameters**: Dictionary with the parameters required by the optimizer.

# How to configure settings.json for multiple settings
Please note that this library is designed to run multiple settings simultaneously, which is essential for thoroughly exploring aggregations and attacks. To achieve this, you can provide a list of elements in the `settings.json` file instead of a single element. Below is an example:
`distribution_parameter: 1.0` -> `distribution_parameter: [1.0, 0.5, 0.0]`